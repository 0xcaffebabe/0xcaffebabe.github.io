---
layout: post
title:  "稳定性治理日志（一）：初面风暴"
date:   2025-3-29T00:00:00
category: 稳定性
tags: ['稳定性']
---

# 稳定性治理日志（一）：初面风暴

> 本故事纯属虚构，如有雷同，纯属巧合。

我是陈麦，自加入稳定性治理小组那一刻起，耳畔便不断回响着那些因系统失控而引发的业务危机与声誉风暴。今日，我亲历了一场技术危机——它像是工业时代的一场暴风雪，在数字海洋中掀起惊涛骇浪。

**17:18 —— 风暴初起**  
正值业务高峰，昏黄的监控屏下突然响起了告警 bot 的连续提示音。那刺耳的报警声仿佛红色预警，宣告着暴风雨即将降临：部分机器的 CPU 使用率急速攀升，逼近 100%，订单操作的成功率则犹如沉默的哀鸣逐步下滑。林工第一时间抓起电话，急促地呼叫小组负责人黄工。我们三人迅速围坐在工位，屏幕上跳动的数据如同跳跃的心电图，直指订单主库——那座原本坚固的数字堤坝，正在迎来不可抗拒的雪崩效应。主流程的 RT 正从几百毫秒瞬间飙升至十几秒，我心中不免浮现出客户咆哮、投诉不断的恶梦场景。

**17:30 —— 危机扩散**  
客户的反馈开始如潮水般涌来。黄工果断尝试杀掉那些执行时间冗长的 SQL 语句，并重启数据库以试图撬动这座失衡的巨轮。但数据库一重启，巨量流量便蜂拥而入，订单库的 CPU 依然不肯松懈地狂飙，直至被彻底压垮。此时，客户群里的咆哮声已成了现场唯一的背景音乐——每一个字都重击着我们的神经，仿佛连空气中也弥漫着紧迫与焦虑。

**17:35 —— 内部的探讨**  
在混乱中，我与林工急速展开排查。“难道是恶意攻击？”“接口 QPS 并未暴涨啊！”“缓存会不会失效了？”种种假设在复盘室内此起彼伏。面对不断扩大的问题，我们内心只剩下一个信念：“先止血，再找根源。”每一秒的讨论、每一次敲击键盘的声音都仿佛在对抗着那逐渐蔓延的灾难。

**18:00 —— 专家登场**  
技术专家杨工紧急上线，与黄工联手对症下药。我们眼前浮现的是一幅凛冽的战场图景：订单库的 CPU、会话数、内存数据同步攀升，仿佛暴风雨中的浪潮不断冲击那岌岌可危的防线。黄工先前的重启操作触发了连锁反应：应用节点纷纷断开数据库连接，k8s 的健康检查如同无情的刽子手，重启 pod 使得可用节点不断减少，外部接口的 500 错误数正不断增加，预示着更大的危机。

**18:20 —— 探寻根源**  
黄工找到了一条可疑的慢 SQL，便交由我与林工联手追踪。我们沉浸于代码与执行计划的世界中——那 SQL 在订单主库上执行了排序聚合操作，扫描了大量数据，问题显而易见。虽然这条 SQL 涉及的模块最近未曾改动，但其背后隐藏的隐患让我们不得不暂时按下思考键，等待更明朗的因果。

**18:45 —— 总监亲临现场**  
群里客户的投诉声已经愈演愈烈，老板也急促地走到我们的工位，质问为何问题迟迟未能恢复。黄工在局促中汇报了当前的状况和已尝试的方案。老板果断决定，请来曾担任技术总监的外援。这位总监迅速审视了现场数据后，指出黄工此前的杀慢 SQL 和重启数据库操作，正如在雪崩边缘用利斧劈砍，触发了连锁反应，将压力集中至尚存的节点上，令问题愈发难控。

**19:07 —— 应对之策**  
技术总监提出了一个大胆的假想：让数据库咬住初期的高压，通过加装代理中间件来缓解高会话数引发的内存飙升。黄工与杨工在他的指导下迅速搭建这一“抗压盾牌”。这一举措如同给数据库披上了一层防护铠甲，订单库的压力终于开始缓慢下降，那不断攀升的红色警戒数字也逐渐退去。

**19:45 —— 转危为安**  
在硬抗与精细调优的双重攻势下，订单库的负载开始回落，系统接口的错误率随之下降。渐渐地，那原本咆哮的监控数据转为低语，仿佛在宣告暴风雨的散去。

**20:08 —— 危机散尽**  
主链路接口的 RT 恢复至历史常态，错误率降至几近零。随着这一刻的到来，我与林工不由自主地又回到了那初探可疑 SQL 的战场。我们发现，该 SQL 正服务于一个首页仪表盘模块，而这一模块的请求量竟达到平时的三倍。进一步推敲，虽有缓存的防护，但仅一分钟的有效期难以抵御突然爆发的流量，正是这场缓存雪崩，令大量请求齐刷刷地打穿了数据库的防线，继而触发一连串连锁反应。于是，我们果断加上了功能开关，暂时关闭这一隐患模块，避免更大的祸端。

**复盘与反思**  
事后，针对这次风暴，我们也进行了反思：
- **应急决策仓促**：小组负责人黄工在高峰时刻选择了迅速杀慢 SQL 和重启数据库的操作，虽想尽快止血，但却引发了雪崩效应。  
- **应急预案不足**：面对不断扩大的问题，应及时向上反馈共享信息，若能更早与技术总监共享现场信息，也许能更快触发正确的应急流程。
- **场景隔离缺失**：类似的数据统计场景应做到 OLAP 与 OLTP 的彻底隔离，否则缓存雪崩便可能引发连锁故障，冲击在线业务。

尽管事后我们深知，每个现场决策都充满艰难与权衡——在业务高峰中，任何拖延都可能酿成更大灾难，而身外局外人的外援总监在业务低谷时的“硬抗”决策，也让我们在这场危机中看到了更多的可能性。

夜幕低垂，电脑屏幕上的数据逐渐静默，我们合上了紧绷的战场日志，走向那略显冰冷的电梯。墙上监控大屏的红绿曲线依然闪烁，仿佛在低语：“风暴已过，但考验永续，熵增永远在呼唤着下一个黎明。”
